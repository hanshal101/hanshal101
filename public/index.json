[{"content":"Motivation In this blog series, we will deep dive into the fascinating world of eBPF (Extended Berkley Packet Filter) and its internals. Along the way, we will explore core BPF (Berkley Packet Filter) internals that are essential to understanding how eBPF operates under the hood. This journey will also include networking concepts, operating system principles, and computer architecture, all of which play a crucial role in making eBPF effective and powerful. Who should read this?: Everyone who\u0026rsquo;s curious about eBPF and Linux Kernel.\nWhat is eBPF? eBPF is a revolutionary kernel technology that allows developers to write custom code that can be loaded into the kernel dynamically, changing the way the kernel behaves. This enables a new generation of highly performant networking, observability, and security tools. And as you\u0026rsquo;ll see, if you want to instrument an app with these eBPF-based tools, you don\u0026rsquo;t need to modify or reconfigure the app in any way, thanks to eBPF\u0026rsquo;s vantage point within the kernel. Just a few of the things you can do with eBPF include:\nPerformance tracing of pretty much any aspect of a system High-performance networking, with built-in visibility Detecting and (optionally) preventing malicious activity History of eBPF Previously our OS was the center for all observability, networking and security functionality due to Kernel\u0026rsquo;s ability to oversee and control entire system. However, evolving these functionalities within the kernel was challenging due to its critical role in maintaining stability and security. This limitation led to slower innovation compared to functionalities developed outside of the operating system. The introduction of eBPF (Extended Berkeley Packet Filter) in 2014 revolutionized this landscape by allowing developers to run sandboxed programs directly within the Linux kernel without modifying its source code or loading additional modules.\nThe Linux Kernel The Linux kernel is the software layer between your applications and the hardware they\u0026rsquo;re running on. Applications run in an unprivileged layer called user space, which can\u0026rsquo;t access hardware directly. Instead, an application makes requests using the system call (syscall) interface to request the kernel to act on its behalf. That hardware access can involve reading and writing to files, sending or receiving network traffic, or even just accessing memory. The kernel is also responsible for coordinating concurrent processes, enabling many applications to run at once.\nA diagram of the Linux Kernel\nHow does eBPF programs run? In a higher level abstraction\neBPF programs can be writtern in languages like C/C++, Go, Rust, Python etc. Developers write eBPF programs primarily in restricted C or other supported languages. These programs are compiled into intermediary bytecode using tools like LLVM or clang. Then the bytecode is loaded into the Linux Kernel via tools like bpftool or through bpf() syscalls. After the program is loaded it goes through a verification process to make sure it is safe to run in the kernel. The verifier will check for potential issues like infinite loops, uninitialized variables, out-of-bound memory access, etc. If verified successfully, the bytecode is converted into native machine code by a JIT (Just-In-Time) compiler(We will discuss about this in later upcoming blogs) for efficient execution. The program then gets attached to specific hookpoints within the kernel. Example: syscalls, network events, Any event occuring at those hook points where the eBPF program is attached, it executes automatically. We can also make the eBPF program to interact with the user-space of the system. This is done for better visibility of the traces. At last once the task is completed or when no longer needed, eBPF programs can be unloaded from the kernel using system calls again.\nA diagram of the eBPF process\nBasic Terminologies in eBPF Maps Remember we discussed about how eBPF program can interact with the user-space? Well MAPS help us to do so. eBPF maps play a crucial role in facilitating data sharing and state management for eBPF programs. These maps enable the storage and retrieval of data using a variety of data structures, allowing eBPF programs to access them directly. Additionally, applications running in user space can also interact with these maps through system calls, providing a seamless way to exchange information between kernel and user space environments. Hash-tables, arrays, Ring-Buffer, LRU (Least Recently Used) are some supported map types. We will discuss in detail about different type of MAPS in upcoming blogs.\nHelper Calls eBPF programs are restricted from calling arbitrary kernel functions to maintain compatibility across different kernel versions. This limitation prevents eBPF programs from becoming tightly coupled with specific kernel releases, which would complicate their portability. Instead, eBPF programs can utilize a set of predefined and stable helper functions provided by the kernel. These helper functions offer a well-defined API that allows eBPF programs to perform various tasks without directly accessing the broader kernel functionality. Few helper calls include:\nGenerate random numbers Get current time \u0026amp; date eBPF map access Get process/cgroup context Manipulate network packets and forwarding logic Tail Calls Tail calls in eBPF allow developers to execute another eBPF program and replace the current execution context, somewhat analogous to how the execve() system call works for regular processes.\nFunction Calls Function calls in eBPF allow developers to define and invoke functions within an eBPF program, enhancing code reusability and modularity.\nWhy eBPF? eBPF is a versatile technology that offers several compelling reasons for its adoption across various domains, including security, observability, and networking\nSecurity eBPF provides fine-grained control over system calls and network activities, allowing for real-time threat detection and mitigation without modifying kernel source code. Its sandboxed environment ensures stability by preventing crashes or security breaches.\nObservability It enables non-invasive monitoring of system events with minimal performance impact. This allows for real-time tracing and customizable monitoring solutions tailored to specific use cases\nPerformance By executing directly within the kernel via JIT compilation, eBPF reduces latency and enhances throughput compared to traditional user-space applications. It also minimizes resource usage due to its lightweight nature.\nWhere is eBPF currently used? eBPF is currently used in a wide range of applications across various domains, including networking, security, and observability. In networking, eBPF is utilized for tasks such as traffic control, creating network policies, and connect-time load balancing in environments like Kubernetes. For security, it is employed for real-time threat detection by monitoring system calls and network activities to identify potential vulnerabilities. Additionally, eBPF enhances observability by providing granular insights into system metrics like CPU utilization and disk I/O operations. It also plays a crucial role in application performance monitoring by tracing micro-level events to optimize application performance. Tools like Cilium leverage eBPF for secure network connectivity and monitoring capabilities in cloud-native environments. Overall, its versatility makes it an essential tool across multiple industries.\n","permalink":"http://localhost:1313/blog/ebpf/introduction-to-ebpf/","summary":"Motivation In this blog series, we will deep dive into the fascinating world of eBPF (Extended Berkley Packet Filter) and its internals. Along the way, we will explore core BPF (Berkley Packet Filter) internals that are essential to understanding how eBPF operates under the hood. This journey will also include networking concepts, operating system principles, and computer architecture, all of which play a crucial role in making eBPF effective and powerful.","title":"An Introduction to eBPF"},{"content":"Motivation eBPF (extended Berkeley Packet Filter) has revolutionized how we implement networking functionality in Linux systems. This comprehensive blog explores the networking concepts involved in eBPF, covering everything from fundamental principles to advanced implementations. With eBPF, developers can run sandboxed programs within the kernel space, providing unprecedented control over network traffic without compromising system stability or requiring kernel modifications.\nIntroduction to eBPF eBPF represents a significant evolution from the original Berkeley Packet Filter. Created initially for filtering network packets, eBPF has expanded into a versatile execution engine that can be leveraged for numerous use cases, with networking remaining one of its most prominent applications.\nThe Evolution of BPF to eBPF The original BPF was designed as a simple packet filtering mechanism, but its capabilities were radically expanded in 2014. The result-extended BPF or eBPF-allows programs broad access to kernel functions and system memory, but in a protected way. This expansion enables eBPF to gather detailed information about low-level networking, security, and other system-level activities within the kernel without requiring direct modifications to kernel code.\nUnlike programs that run in user space, eBPF programs are inherently more efficient and potentially more powerful because they can see and respond to nearly all operations performed by the operating system. For application tracing, eBPF provides the advantage of not requiring any code instrumentation, and since it supports event-driven functions, tracing can be performed efficiently as CPU cycles are used only when needed.\nWhy eBPF is Revolutionary for Networking eBPF has transformed network programming in Linux by:\nProviding built-in hooks for programs based on system calls, kernel functions, network events, and other triggers Offering a mechanism for compiling and verifying code prior to running, ensuring security and stability Enabling a more straightforward way to enhance kernel functionality than is possible through Loadable Kernel Modules (LKMs) These capabilities make eBPF particularly valuable for networking challenges that traditional tools struggle to address efficiently or with sufficient granularity. Because eBPF programs are event-based, they can enable efficient but complex processing of network traffic and design detailed but lightweight security and observability features.\nLinux Networking Fundamentals To understand how eBPF enhances networking, we first need to grasp the fundamentals of Linux networking architecture and how packets flow through the kernel.\nLinux Kernel Networking Stack The Linux networking stack follows a layered model similar to the OSI model but with Linux-specific adaptations. When a network packet arrives, it flows through several layers before reaching user-space applications:\nPhysical Layer: Managed by network interface card (NIC) drivers that handle the physical reception of packets Link Layer: Handles Ethernet frames and addressing Network Layer: Processes IP packets, handles routing decisions Transport Layer: Manages TCP/UDP connections and reliability Socket Interface: Connects kernel networking to userspace applications eBPF can attach to various points within this stack, allowing for unprecedented control over packet processing.\nLinux Kernel Networking Stack\nPacket Flow in Linux Understanding packet flow is crucial for effective eBPF programming. A packet typically follows this path:\nIngress Path:\nPacket arrives at the NIC XDP (eXpress Data Path) hook - earliest point for eBPF interception NIC driver processing Traffic Control (TC) ingress hook - another eBPF interception point Netfilter hooks (used by iptables) Protocol handlers (IP, TCP, UDP) Socket delivery to applications Egress Path:\nApplication sends data through socket Protocol handlers process outgoing data Routing subsystem makes forwarding decisions Netfilter hooks process outgoing packets Traffic Control (TC) egress hook - eBPF interception point NIC driver prepares packet for transmission Packet transmitted by NIC Each of these points offers opportunities for eBPF programs to intercept, analyze, modify, or redirect network traffic.\neBPF Program Types for Networking eBPF provides several program types specifically designed for different networking use cases. Each program type corresponds to a specific attachment point in the networking stack.\nSocket Filter Programs Socket filter programs (BPF_PROG_TYPE_SOCKET_FILTER) represent one of the earliest applications of BPF. These programs can hook into network sockets and are designed to filter or modify packets received by that socket. Importantly, these programs only operate on ingress (received) packets, not outgoing packets.\nSocket filter programs are called by the kernel with a __sk_buff context. The return value from these programs indicates how many bytes of the message should be kept. Returning a value less than the size of the packet will truncate it, and returning 0 will discard the packet entirely.\nA notable use case for this program type is tcpdump, which uses raw sockets in combination with socket filters generated from filter queries to efficiently process only packets of interest, minimizing the cost of kernel-userspace transitions.\nTo use socket filter programs, they are typically placed in an ELF section prefixed with socket and attached to network sockets using the setsockopt syscall with the SOL_SOCKET socket level and SO_ATTACH_BPF socket option.\nTraffic Control (TC) Programs Traffic Control (TC) is Linux\u0026rsquo;s mechanism for controlling packet sending and receiving in terms of rate, sequence, and other aspects. Located at the link layer, TC comes into play after sk_buff allocation and operates later in the processing path than XDP.\nIn the TC subsystem, the corresponding data structure and algorithm control mechanism are abstracted as qdisc (Queueing discipline). It exposes two callback interfaces for enqueuing and dequeuing packets externally while internally hiding the implementation of queuing algorithms.\nTC can implement complex tree structures based on filters and classes:\nFilters are mounted on qdisc or class to implement specific filtering logic Classes organize packets into different categories Actions are executed when packets match specific filters When a packet reaches the top-level qdisc, its enqueue interface is called, and mounted filters are executed sequentially until one matches successfully. The packet is then sent to the class pointed to by that filter and enters the qdisc processing configured for that class.\nThe TC framework with eBPF provides a classifier-action mechanism, allowing an eBPF program loaded as a filter to return values that determine packet handling, implementing a complete packet classification and processing system.\nXDP (eXpress Data Path) Programs XDP programs represent one of the most powerful networking applications of eBPF. They operate at the earliest possible point in the networking stack, before the kernel allocates memory structures for packets. This early interception enables extremely high-performance packet processing.\nXDP programs can return several actions that determine the packet\u0026rsquo;s fate:\nXDP_DROP: Tells the driver to drop packets at an early stage, which is extremely efficient for filtering and DDoS mitigation XDP_PASS: Allows packets to continue to the normal network stack XDP_TX: Forwards packets using the same NIC by which they were received XDP_REDIRECT: Forwards packets to a different network interface or CPU These capabilities make XDP ideal for high-performance networking applications such as:\nDDoS mitigation and firewalling Forwarding and load balancing Network monitoring Protocol translation cGroup Socket Programs cGroup socket programs (BPF_PROG_TYPE_CGROUP_SOCK) are attached to cGroups and triggered when sockets are created, released, or bound by a process in that cGroup. These programs allow for policy enforcement and monitoring at the socket level based on a process\u0026rsquo;s cGroup membership.\nThis program type enables fine-grained control over socket operations for processes in specific cGroups, which is particularly useful in containerized environments.\nXDP: High-Performance Packet Processing XDP (eXpress Data Path) represents one of the most transformative networking features enabled by eBPF, offering unprecedented performance for packet processing applications.\nXDP Architecture and Hook Points XDP operates directly at the driver level, intercepting packets immediately as they arrive from the network interface, before any memory allocations or other kernel processing occurs. This early interception point provides several advantages:\nMinimal Processing Overhead: Packets can be processed or dropped before the kernel allocates memory for them Reduced Latency: Fast-path operations can bypass much of the kernel\u0026rsquo;s networking stack High Throughput: XDP can process millions of packets per second on a single CPU core XDP provides several hook points for attaching eBPF programs:\nDriver/Native XDP: Implemented directly in the network driver, offering the best performance but requiring driver support Generic XDP: Runs later in the stack after the sk_buff allocation, with somewhat reduced performance but broader compatibility Offloaded XDP: Programs are offloaded to the NIC hardware, enabling wire-speed processing without CPU involvement for supported NICs XDP Actions and Use Cases XDP programs can implement various packet handling strategies through several return values:\nXDP_DROP: Instructs the driver to drop the packet immediately, which is extremely efficient for filtering and DDoS mitigation XDP_PASS: Allows the packet to continue to the normal network stack for further processing XDP_TX: Transmits the modified packet back out through the same NIC it arrived on, useful for creating simple packet responders or modifying and returning packets XDP_REDIRECT: Forwards the packet to a different network interface or to a different CPU for processing, enabling advanced use cases like load balancing or packet steering These actions make XDP particularly well-suited for several networking applications:\nDDoS Mitigation and Firewalling One of the fundamental functions of XDP is using XDP_DROP to eliminate unwanted traffic at an early stage. This capability allows for implementing various efficient network security strategies while keeping the processing cost of each packet very low.\nXDP excels at handling DDoS attacks by scrubbing illegitimate traffic and forwarding legitimate packets to their destination using XDP_TX. It can be deployed either in standalone network appliances or distributed to multiple nodes that protect the host.\nFor maximum performance, offloaded XDP can shift processing entirely to the NIC, allowing packets to be processed at wire speed.\nForwarding and Load Balancing XDP enables efficient packet forwarding and load balancing through XDP_TX or XDP_REDIRECT operations. This allows data packets to be manipulated using BPF helper functions to increase or decrease packet headroom, or to encapsulate and decapsulate packets before sending them.\nLoad balancers can be implemented using either:\nXDP_TX to forward packets using the same NIC by which they were received XDP_REDIRECT to forward packets to a different network interface Programming with XDP Creating an XDP program involves several steps:\nSetting up the development environment by installing required packages: sudo dnf install clang llvm gcc libbpf libbpf-devel libxdp libxdp-devel xdp-tools bpftool kernel-headers Writing the XDP program in C, such as a simple program to drop all packets: #include \u0026lt;linux/bpf.h\u0026gt; SEC(\u0026#34;xdp\u0026#34;) int xdp_drop(struct xdp_md *ctx) { return XDP_DROP; } Building the program using Clang: clang -O2 -g -Wall -target bpf -c xdp_drop.c -o xdp_drop.o Loading the program using appropriate tools like bpftool or xdp-loader This straightforward approach allows for rapid development and deployment of high-performance networking applications.\nTraffic Control with eBPF Traffic Control (TC) is Linux\u0026rsquo;s subsystem for controlling packet sending and receiving. With eBPF support, TC has become a powerful platform for implementing complex packet processing logic.\nTC Architecture and Components TC is located at the link layer in the Linux networking stack, operating after sk_buff allocation has been completed. The TC subsystem consists of several components:\nQueueing Disciplines (qdiscs): Algorithms that control how packets are queued and dequeued Classes: Organize packets into categories for different treatment Filters: Match packets based on criteria and assign them to classes Actions: Operations performed on packets when they match filters In the TC subsystem, the corresponding data structure and algorithm control mechanism are abstracted as qdisc (Queueing discipline). It exposes two callback interfaces for enqueuing and dequeuing packets externally while internally hiding the implementation of queuing algorithms.\nFilters and Classes in TC TC can implement complex packet processing through filters and classes:\nFilters are mounted on qdisc or class to implement specific filtering logic Classes organize packets into different categories Actions are executed when packets match specific filters When a packet reaches the top-level qdisc:\nIts enqueue interface is called Mounted filters are executed sequentially until one matches The packet is sent to the class pointed to by that filter The packet enters the qdisc processing configured for that class The TC framework with eBPF provides a classifier-action mechanism that enables both packet classification and processing in an integrated way.\nTC vs. XDP: When to Use Which While both TC and XDP enable programmable packet processing, they have different characteristics that make them suitable for different use cases:\nXDP advantages:\nProcesses packets earlier, before sk_buff allocation Higher performance for simple packet filtering and dropping Better suited for DDoS mitigation and high-throughput applications TC advantages:\nRicher context with full sk_buff access Better integration with existing traffic control mechanisms More suitable for complex packet transformation and manipulation Works with virtual interfaces and in scenarios where XDP is not supported Generally, use XDP for high-performance packet filtering and forwarding, and TC for complex traffic shaping, detailed packet manipulation, and where integration with existing QoS mechanisms is required.\neBPF Maps for Networking eBPF maps are key-value stores that allow data sharing between eBPF programs and between kernel and user space. They are crucial for networking applications, providing state storage, configuration, and inter-program communication.\nMap Types Overview Linux kernel provides numerous map types for various use cases. For networking applications, these can be categorized as:\nGeneric map types: General-purpose storage like hash tables and arrays Map-in-map types: Maps that store references to other maps Streaming maps: For large data transfer between kernel and user space Packet redirection maps: For steering packets between devices, CPUs, or sockets Special-purpose maps: For specific networking functions Hash and Array Maps These fundamental map types provide the building blocks for many networking applications:\nBPF_MAP_TYPE_HASH: A generic hash table for key-value lookups BPF_MAP_TYPE_ARRAY: An array with fixed-size entries, indexed by integers BPF_MAP_TYPE_LRU_HASH: A hash with least-recently-used eviction policy BPF_MAP_TYPE_LPM_TRIE: A longest-prefix match tree, ideal for IP routing tables These maps are commonly used for connection tracking tables, flow state storage, configuration parameters, and statistics collection.\nPer-CPU Maps Per-CPU map variants maintain separate copies of the map for each logical CPU, eliminating the need for synchronization:\nBPF_MAP_TYPE_PERCPU_HASH: Per-CPU version of hash map BPF_MAP_TYPE_PERCPU_ARRAY: Per-CPU version of array map These maps offer superior performance for high-traffic networking applications by eliminating contention between CPUs. Since multiple CPUs never read or write to memory accessed by another CPU, these maps avoid race conditions and the need for synchronization mechanisms like spin-locks or atomic instructions.\nPer-CPU maps also improve performance through better cache locality and can serve as efficient scratch buffers for temporary storage during packet processing.\nSocket Maps and Packet Redirection Maps Specialized maps designed for networking operations:\nBPF_MAP_TYPE_SOCKMAP: Stores socket references for redirection between sockets BPF_MAP_TYPE_SOCKHASH: Hash-based socket storage for efficient lookups BPF_MAP_TYPE_DEVMAP: Stores network device references for XDP redirection between interfaces BPF_MAP_TYPE_CPUMAP: Enables XDP packet redirection between CPUs for balanced processing These maps facilitate packet steering between network devices, CPUs, and sockets, enabling efficient implementation of load balancing, forwarding, and network function virtualization.\nRingbuf and Perf Event Arrays Maps that enable efficient data streaming between kernel and user space:\nBPF_MAP_TYPE_RINGBUF: A ring buffer for efficient bulk data transfer BPF_MAP_TYPE_PERF_EVENT_ARRAY: Uses perf subsystem for event notification For networking applications, these maps are valuable for packet sampling and capture, network telemetry and monitoring, flow record export, and network analytics.\nMap-in-Map Structures Map-in-map types store references to other maps, enabling complex data structures:\nBPF_MAP_TYPE_ARRAY_OF_MAPS: An array where each element is a map BPF_MAP_TYPE_HASH_OF_MAPS: A hash table where values are maps These structures enable sophisticated networking applications like multi-level routing tables, hierarchical policy enforcement, and tenant isolation in multi-tenant networks.\nSocket Filtering and Manipulation Socket filtering is one of the original use cases for BPF and continues to be a powerful application of eBPF. Socket filters allow programs to inspect and filter packets at the socket level, providing an efficient way to process only relevant network traffic.\nSocket Filter Programs Socket filter programs (BPF_PROG_TYPE_SOCKET_FILTER) are designed to filter or modify packets received by network sockets. These programs hook into network sockets but only operate on ingress (received) packets, not egress (outgoing) packets.\nSocket filter programs are called by the kernel with a __sk_buff context, and their return value indicates how many bytes of the message should be kept. Returning a value less than the size of the packet truncates it, while returning 0 discards the packet completely.\nA common use case for socket filters is tcpdump, which uses raw sockets with socket filters generated from filter queries to efficiently process only packets of interest, minimizing the kernel-userspace barrier cost.\nSocket filter programs are typically placed in an ELF section prefixed with socket and attached to network sockets using the setsockopt syscall with SOL_SOCKET socket level and SO_ATTACH_BPF socket option.\nHelper Functions for Socket Programs Socket filter programs can use various helper functions to interact with the system and the context in which they operate. Some of the helper functions available to socket filter programs include:\nbpf_get_socket_cookie bpf_get_socket_uid bpf_ktime_get_ns bpf_map_lookup_elem bpf_map_update_elem bpf_perf_event_output bpf_get_current_pid_tgid bpf_get_current_task These helpers enable socket filter programs to access various information about sockets, processes, and system state, enhancing their capabilities for packet processing and analysis.\nSocket Map Usage eBPF provides special map types for socket operations:\nBPF_MAP_TYPE_SOCKMAP: Stores references to sockets for redirection BPF_MAP_TYPE_SOCKHASH: A hash-based version of SOCKMAP for efficient lookups These maps enable advanced socket operations like fast socket lookup based on connection information, efficient socket redirection, and socket message forwarding between sockets.\nPacket Redirection and Forwarding Packet redirection is a powerful capability of eBPF that allows packets to be steered between interfaces, CPUs, and sockets without traversing the entire networking stack. This capability enables efficient implementation of networking functions like load balancing, forwarding, and NAT.\nInterface Redirection with XDP XDP allows packets to be redirected between network interfaces using the BPF_MAP_TYPE_DEVMAP and the bpf_redirect_map() helper function. This capability enables:\nSoftware-defined networking Virtual switching Service chaining Policy-based routing Interface redirection typically involves:\nDetermining the target interface based on packet attributes or policy Looking up the interface in a DEVMAP Using XDP_REDIRECT action with appropriate helper function CPU Redirection CPU redirection allows packets to be distributed across CPU cores for balanced processing:\nXDP CPU Redirection: Using BPF_MAP_TYPE_CPUMAP and the bpf_redirect_map() helper RSS (Receive Side Scaling): Hardware-based distribution that can be influenced by eBPF CPU redirection enables load balancing across cores, processor affinity for related flows, and optimization of cache locality.\nImplementing Load Balancing XDP is particularly well-suited for implementing load balancers:\nL3/L4 Load Balancing: Based on IP addresses and ports Extract flow information (IPs, ports) Compute consistent hash Select backend using hash Redirect to selected backend using XDP_TX or XDP_REDIRECT L7 Load Balancing: Based on application-layer information Parse HTTP/gRPC/etc. headers Apply load balancing logic based on content Redirect to appropriate backend Load balancers can be implemented using either:\nXDP_TX to forward packets using the same NIC by which they were received XDP_REDIRECT to forward packets to a different network interface Network Monitoring and Observability eBPF provides unprecedented capabilities for network monitoring and observability, enabling detailed visibility into network behavior without modifying applications or introducing significant overhead.\nTracing Network Functions eBPF can trace network-related kernel functions to provide insights into network behavior:\nKprobe-based tracing: Attaching eBPF programs to entry and exit points of kernel networking functions Tracepoint-based tracing: Using predefined tracepoints in the networking stack Raw tracepoints: Lower-overhead alternatives to standard tracepoints These tracing mechanisms enable detailed function call tracking, parameter inspection, performance analysis, and debugging of network issues.\nSocket Monitoring eBPF programs can monitor socket operations to provide insights into application network behavior:\nSocket creation and binding: Tracking when sockets are created and bound to addresses Connection establishment: Monitoring TCP connection setup and teardown Data transfer: Measuring throughput and patterns of data flow Error conditions: Detecting socket errors and failures This monitoring provides valuable insights into application networking behavior and performance.\nFlow Monitoring eBPF facilitates detailed flow monitoring at wire speed:\nFlow identification: Based on 5-tuple (IPs, ports, protocol) Flow statistics: Packets, bytes, duration Flow behavior analysis: Patterns, protocols, periodicity Flow sampling: Efficient collection of representative data Flow monitoring applications include:\nTraffic accounting Anomaly detection Capacity planning Application dependency mapping Security Applications of eBPF Networking eBPF has revolutionized network security by enabling programmable, high-performance security functions directly in the kernel.\nFirewalling with eBPF eBPF enables next-generation firewall capabilities:\nXDP-based firewalling: Ultra-fast packet filtering at the driver level, ideal for volumetric attack mitigation Stateful firewalling: Using eBPF maps to track connection state Dynamic rule updates: Modifying firewall behavior without service disruption Using XDP_DROP with eBPF allows for implementing firewall policies with very little overhead per packet, making it extremely efficient for filtering large volumes of traffic.\nDDoS Mitigation Strategies eBPF is particularly effective for DDoS mitigation:\nEarly packet dropping: Using XDP to drop attack traffic before it consumes system resources Traffic classification: Distinguishing legitimate from attack traffic Rate limiting: Implementing per-source rate limiting to contain attacks XDP can handle DDoS scenarios by scrubbing illegitimate traffic and forwarding legitimate packets to their destination using XDP_TX. This approach can be implemented either in standalone network appliances or distributed across multiple nodes that protect the host.\nFor maximum performance, offloaded XDP can shift processing entirely to the NIC, allowing packets to be processed at wire speed.\nConclusion In this first part of our deep dive into networking with eBPF, we explored the fundamental building blocks that make eBPF a game-changer for modern networking on Linux. Starting with a historical evolution from classic BPF to the powerful eBPF framework, we examined how eBPF empowers developers to gain unprecedented control over packet flow within the kernel without compromising stability or requiring kernel modifications.\nWe navigated the Linux networking stack to understand where and how eBPF programs can be attached. We covered various eBPF program types such as socket filters, traffic control programs, XDP for high-performance packet processing, and cGroup socket programs. Each of these offers unique capabilities tailored to different networking needs.\nWe also delved into the rich landscape of eBPF maps that underpin stateful packet processing and data sharing, from basic hash maps to advanced per-CPU and redirection maps. Furthermore, we covered packet redirection strategies, load balancing techniques, observability through tracing and monitoring, and security applications including DDoS mitigation and firewalling—all achieved using eBPF’s flexible and performant architecture.\neBPF is redefining what’s possible in networking—bringing high throughput, fine-grained control, and programmability directly into the kernel. In the next part of this series, we’ll go deeper into real-world use cases, practical implementation patterns, and performance tuning techniques to harness the full potential of eBPF in production environments.\n","permalink":"http://localhost:1313/blog/ebpf/networking-with-ebpf/","summary":"Motivation eBPF (extended Berkeley Packet Filter) has revolutionized how we implement networking functionality in Linux systems. This comprehensive blog explores the networking concepts involved in eBPF, covering everything from fundamental principles to advanced implementations. With eBPF, developers can run sandboxed programs within the kernel space, providing unprecedented control over network traffic without compromising system stability or requiring kernel modifications.\nIntroduction to eBPF eBPF represents a significant evolution from the original Berkeley Packet Filter.","title":"Networking with eBPF: From Fundamentals to Advanced Applications"},{"content":"Description Developed few API\u0026rsquo;s regarding the monitoring of hardware application in GoLang. Collaborated with the Data team for creating visualizations for the collected data. Improved pipelines and implemented best practices for Containerization of the application. ","permalink":"http://localhost:1313/experience/new-leap-initiative/","summary":"Description Developed few API\u0026rsquo;s regarding the monitoring of hardware application in GoLang. Collaborated with the Data team for creating visualizations for the collected data. Improved pipelines and implemented best practices for Containerization of the application. ","title":"Software Engineer Trainee"},{"content":"Description ","permalink":"http://localhost:1313/experience/cloudraft/","summary":"Description ","title":"DevOps/SRE Intern"}]